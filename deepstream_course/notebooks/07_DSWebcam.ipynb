{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0  Live Stream\n",
    "### USB WEBCAM CONNECTED TO JETSON REQUIRED\n",
    "\n",
    "In the examples presented so far, the input stream has been a file, played as a stream.  In this notebook, you'll use a live stream using a webcam. Attach a USB webcam to your Jetson Nano using an available USB port.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/06_example.png\" alt=\"webcam input\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[7.1 Build a Pipeline with Webcam Input](#7.1-Build-a-Pipeline-with-Webcam-Input)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[7.1.1 Practice Application `deepstream-test1-usbcam-rtsp-out`](#7.1.1-Practice-Application-deepstream-test1-usbcam-rtsp-out)<br>\n",
    "&nbsp; &nbsp; &nbsp;[7.1.2 Exercise: Run the Base Application](#7.1.2-Exercise:-Run-the-Base-Application)<br>\n",
    "**[7.2 Change the Network to YOLO](#7.2-Change-the-Network-to-YOLO)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[7.2.1 Exercise: Run YOLO with a Webcam](#7.2.1-Exercise:-Run-YOLO-with-a-Webcam)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7.1 Build a Pipeline with Webcam Input\n",
    "In all of our applications so far, we have used files as our input sources.  The pipelines processed these files in two ways before passing the streams to the `Gst-nvstreammux` plugin:\n",
    "\n",
    "1. Single file with known format as in the `deepstream-test1` and `deepstream-test2` sample apps:\n",
    "  - `GstFileSrc` - reads the video data from file\n",
    "  - `GstH264Parse` - parses the incoming H264 stream\n",
    "  - `Gst-nvv4l2decoder` - hardware accelerated decoder; decodes video streams using NVDEC\n",
    "  - `Gst-nvstreammux` - batch video streams before sending for AI inference   \n",
    "  \n",
    "2. Multistream input using URI with run-time determination of format as in the `deepstream-test3` sample app; note that we could have used URI streams other than \"file\" in this case:\n",
    "  - `uridecoderbin` source bins - one or more instances created at runtime to read the video data\n",
    "  - `Gst-nvstreammux` - batch video streams before sending for AI inference   \n",
    "\n",
    "For the webcam input, the `deepstream-test1-usbcam` sample app uses a different set of plugins at the beginning of the pipeline. \n",
    "\n",
    "3. Video capture device input with conversion:\n",
    "  - `GstV4l2Src` - can be used to capture video from v4l2 devices, like webcams and tv cards\n",
    "  - `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "  - `GstVideoConvert` - Convert video frames between a great variety of video formats\n",
    "  - `Gst-nvvideoconvert` - performs video color format conversion (I420 to RGBA)\n",
    "  - `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "  - `Gst-nvstreammux` - batch video streams before sending for AI inference\n",
    "\n",
    "We'll begin with the `deepstream-test1-usbcam-rtsp-out` [course application](deepstream/sources/deepstream_python_apps/dli_apps/deepstream-test1-usbcam-rtsp-out/deepstream_test_1_usb.py), which is a combination of the `deepstream-test1-usbcam` and `deepstream-test1-rstp-out` DeepStream Python sample applications (found at `deepstream/sources/deepstream_python_apps/apps`). \n",
    "\n",
    "Here's a look at the pipeline elements created in the application up to `nvstreammux`, which is common to all of our apps:\n",
    "```python\n",
    "    print(\"Creating Source \\n \")\n",
    "    source = Gst.ElementFactory.make(\"v4l2src\", \"usb-cam-source\")\n",
    "    if not source:\n",
    "        sys.stderr.write(\" Unable to create Source \\n\")\n",
    "\n",
    "    caps_v4l2src = Gst.ElementFactory.make(\"capsfilter\", \"v4l2src_caps\")\n",
    "    if not caps_v4l2src:\n",
    "        sys.stderr.write(\" Unable to create v4l2src capsfilter \\n\")\n",
    "\n",
    "    # videoconvert to make sure a superset of raw formats are supported\n",
    "    vidconvsrc = Gst.ElementFactory.make(\"videoconvert\", \"convertor_src1\")\n",
    "    if not vidconvsrc:\n",
    "        sys.stderr.write(\" Unable to create videoconvert \\n\")\n",
    "\n",
    "    # nvvideoconvert to convert incoming raw buffers to NVMM Mem (NvBufSurface API)\n",
    "    nvvidconvsrc = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor_src2\")\n",
    "    if not nvvidconvsrc:\n",
    "        sys.stderr.write(\" Unable to create Nvvideoconvert \\n\")\n",
    "\n",
    "    caps_vidconvsrc = Gst.ElementFactory.make(\"capsfilter\", \"nvmm_caps\")\n",
    "    if not caps_vidconvsrc:\n",
    "        sys.stderr.write(\" Unable to create capsfilter \\n\")\n",
    "\n",
    "    # Create nvstreammux instance to form batches from one or more sources.\n",
    "    streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "    if not streammux:\n",
    "        sys.stderr.write(\" Unable to create NvStreamMux \\n\")\n",
    "\n",
    "```    \n",
    "\n",
    "In summary, the pipeline for this app consists of the following plugins (ordered):\n",
    "\n",
    "- `GstV4l2Src` - can be used to capture video from v4l2 devices, like webcams and tv cards\n",
    "- `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "- `GstVideoConvert` - Convert video frames between a great variety of video formats\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (I420 to RGBA)\n",
    "- `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "- `Gst-nvstreammux` - batch video streams before sending for AI inference\n",
    "- `Gst-nvinfer` - runs inference using TensorRT\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (I420 to RGBA)\n",
    "- `Gst-nvdsosd` - draw bounding boxes, text and region of interest (ROI) polygons\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (RGBA to I420)\n",
    "- `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "- `Gst-nvv4l2h264enc` - encodes RAW data in I420 format to H264\n",
    "- `GstRtpH264Pay` - converts H264 encoded Payload to RTP packets (RFC 3984)\n",
    "- `GstUDPSink` - sends UDP packets to the network. When paired with RTP payloader (`Gst-rtph264pay`) it can implement RTP streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1.1 Practice Application `deepstream-test1-usbcam-rtsp-out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "A sample app, based on the `deepstream-test1` reference app, receives input from a USB webcam and outputs to an RTSP stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some path locations for readability\n",
    "PYTHON_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/apps'\n",
    "STREAMS = '/opt/nvidia/deepstream/deepstream/samples/streams'\n",
    "DLI_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/dli_apps'\n",
    "\n",
    "# List the contents of the sample app\n",
    "!ls $DLI_APPS/deepstream-test1-usbcam-rtsp-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.1.2 Exercise: Run the Base Application\n",
    "Plug in your webcam and execute the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check usage of the test3 mp4 app with the help option\n",
    "!cd $DLI_APPS/deepstream-test1-usbcam-rtsp-out \\\n",
    "    && python3 deepstream_test_1_usb.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the DeepStream app\n",
    "Open the VLC media player on your laptop:\n",
    "- Pull down the \"Media\" menu and select the \"Open Network Stream\" dialog.\n",
    "- Set the URL to `rtsp://192.168.55.1:8554/ds-test`.\n",
    "- Optionally, add a wait delay to VLC:\n",
    "   - Click \"Show more options\" in the dialog.\n",
    "   - Add ` :ipv4=120000` to the \"Edit Options\" line to add a 120 second delay.\n",
    "- Start execution of the cell below.\n",
    "- Click \"Play\" on your VLC media player *after* you start the cell execution.  \n",
    "\n",
    "The stream will start from the Jetson Nano and display in the media player.  There is a delay while the model `.engine` file is built.  \n",
    "\n",
    "If VLC fails, start it again. Close the VLC fail notice and press the \"play\" triangle.\n",
    "\n",
    "To end the stream, use the pull-down menu to interrupt the kernel with *Kernel->Interrupt Kernel*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the app\n",
    "!cd $DLI_APPS/deepstream-test1-usbcam-rtsp-out \\\n",
    "    && python3 deepstream_test_1_usb.py -i /dev/video0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the app is working properly, bring an object into the webcam frame that is either a vehicle, person, bicycle, or road sign, as that is what the PGIE model we're using recognizes.  In the image shown here, the webcam was pointed at a car magazine website for testing.\n",
    "\n",
    "<img src=\"images/06_usb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7.2 Change the Network to YOLO\n",
    "If you set up the YOLO network in the optional [Using Different Networks](05_DiffNetworks.ipynb) notebook, you can change models and use YOLO to identify many more objects with your webcam (up to 80!).  The `deepstream-test1-usbcam-yolo-rtsp-out` application is very similar to the `deepstream-test1-usbcam-rtsp-out` app, but configured for YOLO. It uses the YOLO config file and references that from the Python file.  In addition, variables and constants related to the previous model have been commented away. To see the actual difference in lines, execute the following `diff` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $DLI_APPS \\\n",
    "    && diff deepstream-test1-usbcam-rtsp-out/deepstream_test_1_usb.py \\\n",
    "            deepstream-test1-usbcam-yolo-rtsp-out/deepstream_test_1_usb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7.2.1 Exercise: Run YOLO with a Webcam\n",
    "1. Ensure that you have YOLO installed.  \n",
    "   - If you need to reinstall YOLO, connect your Jetson to the Internet and execute the cells in the [Using Different Networks](05_DiffNetworks.ipynb) notebook up to the first exercise.\n",
    "1. Execute the application as before, using your RTSP viewer to see the webcam stream.  \n",
    "   - There will be a delay while the `.engine` file is built and you may need to restart the RTSP viewer.  \n",
    "1. Try pointing your webcam at various objects to see if the object detector recognizes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the app\n",
    "!cd $DLI_APPS/deepstream-test1-usbcam-yolo-rtsp-out \\\n",
    "    && python3 deepstream_test_1_usb.py -i /dev/video0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "The YOLO network has more objects it can detect, but you may have to experiment a bit with the threshold levels in the configuration file for optimal results for your environment and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/06_usb_yolo.png\" alt=\"file output YOLO\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've completed all the notebooks.  Be sure to work through the assessment questions in the online portion of the DLI course to get your certificate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
