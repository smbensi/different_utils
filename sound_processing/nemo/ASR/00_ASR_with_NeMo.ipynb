{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to End-To-End Automatic Speech Recognition\n",
    "\n",
    "We'll be using the [AN4 dataset from CMU](http://www.speech.cs.cmu.edu/databases/an4/)(with processing using `sox`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ASR ?\n",
    "\n",
    "**Automatic Speech Recognition**, refers to the problem of getting a program to automatically transcribe spoken language (speech-to-text). Our goal is usually to have a model that minimizes the **Word Error Rate (WER)** metric when transcribing speech input. \n",
    "\n",
    "Traditional speech recognition takes a generative approach, modeling the full pipeline of how speech sounds are produced in order to evaluate a speech sample. We would start from a **language model** that encapsulates the most likely orderings of words that are generated (e.g an n-gram model), to a **pronunciation model** for each word in that ordering (e.g. a pronunciation table), to an **acoustic model** that translates those pronunciations to audio waveforms (e.g. a gaussian mixture model).\n",
    "\n",
    "Then, if we receive some spoken input, our goal would be to find the most likely sequence of text that would result in the given audio  according to our generative pipeline of models. Overall, with traditional speech recognition, we try to model `Pr(audio|transcript)*Pr(transcript)`, and take the argmax of this over the possible transcripts.\n",
    "\n",
    "We can see the appeal of **End-To-End ASR architectures**: discriminative models that simply take the audio input and give a textual output, andf in which all the components are trained together towards the same goal. The model's encoder would be akin to an acoustic model for extracting speech features, which can then be directly piped to a decoder which output texts. If desired, we could integrate a language model that would improve our predictions.\n",
    "\n",
    "|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End ASR\n",
    "\n",
    "we want to directly learn `Pr(transcript|audio)` in order to predict the transcripts from the original audio. Since we are dealing with sequential info RNNs are obvious choice. But now we have a pressing problem to deal with: since our input sequence (number of audio timesteps) is not the same length as our desired output (transcript length), how do we match each timestep from the audio  data to the correct output characters ?\n",
    "\n",
    "Earlier speech recognition approaches relied on **temporally-aligned data**, in which each segment of time in an audio file was matched up to a corresponding speech sound such as phoneme or word. However, if we would like to have the flexibility to predict letter-by-letter to prevent OOV (out of vocabulary) issues, then each time step in the data would have to be labeled with the letter sound that the speaker is making at that point in the audio file. With that information, it seems like we should simply be able to try to predict the correct letter for each time step and then collapse the repeated letters (`LLAAAAPPTOOOOP` -> `LAPTOP`) . It turns out that this idea has some problems: not only does alignment make the dataset incredibly labor-intensive to label, but also, what do we do  with words like 'books' that contain consecutive repeated letters ? \n",
    "\n",
    "Modern end-to-end approaches get around this using methods that don't require manual alignment at all, so that the input-output pairs are really just the raw audio and the \n",
    "transcript - no extra data or labelling required. 2 popular approaches to do this: Connectionist Temporal Classification (CTC) and sequence-to-sequence models with attention\n",
    "\n",
    "## CTC\n",
    "\n",
    "In normal speech recognition prediction output, we would expect to have characters such as the letters from A through Z, numbers 0 through 9, spaces (\"_\") and so on. CTC introduces a new intermediate output token called the **blank token** (\"-\") that is useful getting around the alignment problem. \n",
    "\n",
    "With CTC, we still predict one token per time segment of speech, but we use the blank token to figure out where we can and can't collapse the predictions. The appearance of a blank token helps separate repeating letters that should be collapsed. For instance, with an audio snippet segment into `T=11` time steps, we could get predictions that look like `BOO-OOO--KK`, which would then collapse to `BO-O-K` and then we would remove the blank tokens to get our final output `BOOK`.\n",
    "\n",
    "Now, we can predict one output token per time step, then collapse and clean to get sensible output without any fear of ambiguity from repeating letters! A simple way of getting predictions like this would be to apply a bidirectionnal RNN to the audio input, apply softmax over each time step's output, and then take the token with the highest probability. The method of taking always the best token at each time step is called **greedy decoding, or max decoding**\n",
    "\n",
    "\n",
    "To calculate the loss for backprop, we would like to know the log probability of the model producing the correct transcript, `log(Pr(transcript|audio))`. We can get the log probability of a single intermediate output sequence (e.g. `BOO-OOO--KK`) by summing over the log probabilities we get from each token's softmax value, but note that the resulting sum is different from the log probability of the transcript itself (`BOOK`). This is because there are multiple possible output sequences of the same length that can be collapsed to the same transcript and so we need to **marginalize over every valid sequence of length `T` that collapse to the transcript**\n",
    "\n",
    "Therefore, to get our transcript's log proba given our audio input, we must sum the log probabilities if every sequence of length `T` that collapses to the transcript. In practice, we can use a dynamic programming approach to calculate this, accumulating our log probabilities over different \"paths\" through the softmax outputs at each time step.\n",
    "\n",
    "## Sequence-to-Sequence with Attention\n",
    "\n",
    "One problem with CTC is that predictions at different time steps are conditionally independent, which is an issue because the words in a continuous utterance tend to be related to each other in some sensible way. With this conditional independence assumption, we can't learn a language model that can represent such dependencies, though we can add a language model on top of the CTC output to mitigate this to some degree\n",
    "\n",
    "A popular alternative is to use sequence-to-sequence model with attention. A typical seq2seq model for ASR consists of some sort of **bidirectional RNN encoder** that consume the audio sequence timestep-by-timestep, and where the output are then passed to an **attention-based decoder**. Each prediction from the decoder is based on attending to some parts of the entire encoded input, as well as the previously outputted tokens.\n",
    "\n",
    "The outputs of the decoder can be anything from word pieces to phonemes to letters, and since predictions are not directly tied to time steps of the input, we can just continue producing tokens one-by-one until an end token is given (or we reach a specified max output length). This way, we do not need to deal with audio alignment, and our predicted transcript is just the sequence of the outputs given by our decoder.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Look at Our Data (AN4)\n",
    "\n",
    "The AN4 dataset, also known as the Alphanumeric dataset, was collected and published by Carnegie Mellon University. It consists of recordings of people spelling out addresses, names, telephone numbers, etc., one letter or number at a time, as well as their corresponding transcripts. We choose to use AN4 for this tutorial because it is relatively small, with 948 training and 130 test utterances, and so it trains quickly.\n",
    "\n",
    "Before we get started, let's download and prepare the dataset. The utterances are available as `.sph` files, so we will need to convert them to `.wav` for later processing. If you are not using Google Colab, please make sure you have [Sox](http://sox.sourceforge.net/) installed for this step--see the \"Downloads\" section of the linked Sox homepage. (If you are using Google Colab, Sox should have already been installed in the setup cell at the beginning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "data_dir = 'data/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "Dataset downloaded at data//an4_sphere.tar.gz\n",
      "Converting .sph to .wav..\n",
      "Finished conversion. \n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os \n",
    "import subprocess\n",
    "import tarfile \n",
    "import wget\n",
    "\n",
    "# Download the dataset\n",
    "print(\"*****\")\n",
    "if not os.path.exists(data_dir + 'an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url,data_dir)\n",
    "    print(f\"Dataset downloaded at {an4_path}\" )\n",
    "else:\n",
    "    print(\"Tarfile already exists\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'\n",
    "\n",
    "if not os.path.exists(data_dir + '/an4/'):\n",
    "    # untar and convert .sph to .wav (using sox)\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "    print('Converting .sph to .wav..')\n",
    "    sph_list = glob.glob(data_dir + 'an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + \".wav\"\n",
    "        cmd = ['sox', sph_path, wav_path]\n",
    "        subprocess.run(cmd)\n",
    "print(\"Finished conversion. \\n*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
