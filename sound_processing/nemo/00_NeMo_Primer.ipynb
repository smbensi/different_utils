{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio>=0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nemo\n",
    "nemo.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nemo Collections\n",
    "\n",
    "NeMo is sub-divided into a few fundamental collections based on their domains `asr`,`nlp`,`tts`. NeMo allows partial imports of just one or more collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-19 14:05:58 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-19 14:05:58 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-19 14:05:58 experimental:27] Module <class 'nemo.collections.tts.models.vits.VitsModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "from torch import inf\n",
    "import nemo.collections.asr as nemo_asr\n",
    "# import nemo.collections.nlp as nemo_nlp  # problem with torch._six and torch2.0\n",
    "import nemo.collections.tts as nemo_tts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo models in Collections\n",
    "\n",
    "Nemo contains several models for each of its collections, pertaining to certain common tasks involved in conversational AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASRModel',\n",
       " 'AudioToAudioModel',\n",
       " 'EncDecCTCModel',\n",
       " 'EncDecClassificationModel',\n",
       " 'EncDecDiarLabelModel',\n",
       " 'EncDecHybridRNNTCTCBPEModel',\n",
       " 'EncDecHybridRNNTCTCModel',\n",
       " 'EncDecK2SeqModel',\n",
       " 'EncDecRNNTBPEModel',\n",
       " 'EncDecRNNTModel',\n",
       " 'EncDecSpeakerLabelModel',\n",
       " 'EncMaskDecAudioToAudioModel',\n",
       " 'SLUIntentSlotBPEModel',\n",
       " 'SpeechEncDecSelfSupervisedModel']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_models = [model for model in dir(nemo_asr.models) if model.endswith(\"Model\")]\n",
    "asr_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlignerModel',\n",
       " 'FastPitchModel',\n",
       " 'GriffinLimModel',\n",
       " 'HifiGanModel',\n",
       " 'MelPsuedoInverseModel',\n",
       " 'MixerTTSModel',\n",
       " 'RadTTSModel',\n",
       " 'SpectrogramEnhancerModel',\n",
       " 'Tacotron2Model',\n",
       " 'TwoStagesModel',\n",
       " 'UnivNetModel',\n",
       " 'VitsModel',\n",
       " 'WaveGlowModel']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_models = [model for model in dir(nemo_tts.models) if model.endswith(\"Model\")]\n",
    "tts_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nemo Model\n",
    "\n",
    "There are many ways we can create these models - we can use the constructor and pass in a config, we can instantiate the model from a pre-trained checkpoint, or simply pas a pre-trained model name and instantiate a model directly from the cloud\n",
    "\n",
    "Let's try to work with an ASR model [Citrinet](https://arxiv.org/abs/2104.01721)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-19 14:27:26 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo to /home/mat/.cache/torch/NeMo/NeMo_1.16.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo\n",
      "[NeMo I 2023-03-19 14:27:59 common:913] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2023-03-19 14:28:01 mixins:170] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-19 14:28:02 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    \n",
      "[NeMo W 2023-03-19 14:28:02 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2023-03-19 14:28:02 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 12\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-19 14:28:02 features:286] PADDING: 16\n",
      "[NeMo I 2023-03-19 14:28:03 save_restore_connector:247] Model EncDecCTCModelBPE was successfully restored from /home/mat/.cache/torch/NeMo/NeMo_1.16.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo.\n"
     ]
    }
   ],
   "source": [
    "citrinet = nemo_asr.models.EncDecCTCModelBPE.from_pretrained('stt_en_citrinet_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name              | Type                              | Params\n",
       "------------------------------------------------------------------------\n",
       "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
       "1 | encoder           | ConvASREncoder                    | 36.3 M\n",
       "2 | decoder           | ConvASRDecoder                    | 657 K \n",
       "3 | loss              | CTCLoss                           | 0     \n",
       "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
       "5 | _wer              | WERBPE                            | 0     \n",
       "------------------------------------------------------------------------\n",
       "37.0 M    Trainable params\n",
       "0         Non-trainable params\n",
       "37.0 M    Total params\n",
       "147.977   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citrinet.summarize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config using OmegaConf\n",
    "\n",
    "---\n",
    "\n",
    "First we import [`OmegaConf`](https://omegaconf.readthedocs.io/en/latest/). It's an excellent library that is used throughout NeMo in order to enable us to perform yaml configuration management more easily.Additionally, it plays well with another library, `Hydra` that is used by NeMo to perform on the fly config edits from the command line, dramatically boosting ease of use of our config files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All nemo models come packaged with their model configuration inside the `cfg` attribute. While technically it is meant to be config declaration of the model as it has been currently constructed, `cfg` is an essential tool to modify the behaviour of the Model after it has been constructed. it can be safely used to make it easier to perform many essentials tasks inside Models.\n",
    "\n",
    "To be double sure, we generally work on a copy of the config until we are ready to edit it inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 16000\n",
      "train_ds:\n",
      "  manifest_filepath: null\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  trim_silence: true\n",
      "  max_duration: 16.7\n",
      "  shuffle: true\n",
      "  is_tarred: false\n",
      "  tarred_audio_filepaths: null\n",
      "validation_ds:\n",
      "  manifest_filepath: null\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  shuffle: false\n",
      "test_ds:\n",
      "  manifest_filepath:\n",
      "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  shuffle: false\n",
      "  num_workers: 12\n",
      "  pin_memory: true\n",
      "model_defaults:\n",
      "  repeat: 5\n",
      "  dropout: 0.0\n",
      "  separable: true\n",
      "  se: true\n",
      "  se_context_size: -1\n",
      "tokenizer:\n",
      "  dir: /home/smajumdar/PycharmProjects/nemo-eval/nemo_beta_eval/asrset/manifests/asrset_1.4/tokenizers/no_appen/tokenizer_spe_unigram_v1024/\n",
      "  type: bpe\n",
      "preprocessor:\n",
      "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "  sample_rate: 16000\n",
      "  normalize: per_feature\n",
      "  window_size: 0.025\n",
      "  window_stride: 0.01\n",
      "  window: hann\n",
      "  features: 80\n",
      "  n_fft: 512\n",
      "  frame_splicing: 1\n",
      "  dither: 1.0e-05\n",
      "  pad_to: 16\n",
      "  stft_conv: false\n",
      "spec_augment:\n",
      "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "  freq_masks: 2\n",
      "  time_masks: 10\n",
      "  freq_width: 27\n",
      "  time_width: 0.05\n",
      "encoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "  feat_in: 80\n",
      "  activation: relu\n",
      "  conv_mask: true\n",
      "  jasper:\n",
      "  - filters: 512\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 5\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 11\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 13\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 15\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 17\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 19\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 21\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 13\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 15\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 17\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 19\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 21\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 23\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 25\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 25\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 27\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 29\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 31\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 35\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 37\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 39\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 640\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 41\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "decoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
      "  feat_in: 640\n",
      "  num_classes: 1024\n",
      "  vocabulary:\n",
      "  - <unk>\n",
      "  - s\n",
      "  - ▁the\n",
      "  - t\n",
      "  - ▁a\n",
      "  - ▁i\n",
      "  - ''''\n",
      "  - ▁and\n",
      "  - ▁to\n",
      "  - ed\n",
      "  - d\n",
      "  - ▁of\n",
      "  - e\n",
      "  - ▁in\n",
      "  - ing\n",
      "  - .\n",
      "  - ▁it\n",
      "  - ▁you\n",
      "  - 'n'\n",
      "  - ▁that\n",
      "  - m\n",
      "  - 'y'\n",
      "  - er\n",
      "  - ▁he\n",
      "  - re\n",
      "  - r\n",
      "  - ▁was\n",
      "  - ▁is\n",
      "  - ▁for\n",
      "  - ▁know\n",
      "  - a\n",
      "  - p\n",
      "  - c\n",
      "  - ','\n",
      "  - ▁be\n",
      "  - o\n",
      "  - ▁but\n",
      "  - ▁they\n",
      "  - g\n",
      "  - ▁so\n",
      "  - ly\n",
      "  - b\n",
      "  - ▁s\n",
      "  - ▁yeah\n",
      "  - ▁we\n",
      "  - ▁have\n",
      "  - ▁re\n",
      "  - ▁like\n",
      "  - l\n",
      "  - ▁on\n",
      "  - ll\n",
      "  - u\n",
      "  - ▁with\n",
      "  - ▁do\n",
      "  - al\n",
      "  - ▁not\n",
      "  - ▁are\n",
      "  - or\n",
      "  - ar\n",
      "  - le\n",
      "  - ▁this\n",
      "  - ▁as\n",
      "  - es\n",
      "  - ▁c\n",
      "  - ▁de\n",
      "  - f\n",
      "  - in\n",
      "  - i\n",
      "  - ve\n",
      "  - ▁uh\n",
      "  - ent\n",
      "  - ▁or\n",
      "  - ▁what\n",
      "  - ▁me\n",
      "  - ▁t\n",
      "  - ▁at\n",
      "  - ▁my\n",
      "  - ▁his\n",
      "  - ▁there\n",
      "  - w\n",
      "  - ▁all\n",
      "  - ▁just\n",
      "  - h\n",
      "  - ▁can\n",
      "  - ri\n",
      "  - il\n",
      "  - k\n",
      "  - ic\n",
      "  - ▁e\n",
      "  - ▁\n",
      "  - ▁um\n",
      "  - ▁don\n",
      "  - ▁b\n",
      "  - ▁had\n",
      "  - ch\n",
      "  - ation\n",
      "  - en\n",
      "  - th\n",
      "  - ▁no\n",
      "  - ▁she\n",
      "  - it\n",
      "  - ▁one\n",
      "  - ▁think\n",
      "  - ▁st\n",
      "  - ▁if\n",
      "  - ▁from\n",
      "  - ter\n",
      "  - ▁an\n",
      "  - an\n",
      "  - ur\n",
      "  - ▁out\n",
      "  - 'on'\n",
      "  - ▁go\n",
      "  - ck\n",
      "  - ▁would\n",
      "  - ▁were\n",
      "  - ▁w\n",
      "  - ▁will\n",
      "  - ▁about\n",
      "  - ▁right\n",
      "  - ment\n",
      "  - ▁her\n",
      "  - te\n",
      "  - ion\n",
      "  - ▁well\n",
      "  - ▁by\n",
      "  - ce\n",
      "  - ▁g\n",
      "  - ▁oh\n",
      "  - ▁up\n",
      "  - ro\n",
      "  - ra\n",
      "  - ▁when\n",
      "  - ▁some\n",
      "  - ▁also\n",
      "  - ▁their\n",
      "  - ers\n",
      "  - ow\n",
      "  - ▁more\n",
      "  - ▁time\n",
      "  - ate\n",
      "  - ▁has\n",
      "  - ▁people\n",
      "  - ▁see\n",
      "  - ▁pa\n",
      "  - el\n",
      "  - ▁get\n",
      "  - ▁ex\n",
      "  - ▁mean\n",
      "  - li\n",
      "  - ▁really\n",
      "  - v\n",
      "  - ▁ra\n",
      "  - ▁been\n",
      "  - ▁said\n",
      "  - '-'\n",
      "  - la\n",
      "  - ge\n",
      "  - ▁how\n",
      "  - ▁po\n",
      "  - ir\n",
      "  - ▁mo\n",
      "  - ▁who\n",
      "  - ▁because\n",
      "  - ▁co\n",
      "  - ▁other\n",
      "  - ▁f\n",
      "  - id\n",
      "  - ol\n",
      "  - ▁un\n",
      "  - ▁now\n",
      "  - ▁work\n",
      "  - ist\n",
      "  - us\n",
      "  - ▁your\n",
      "  - ▁them\n",
      "  - ver\n",
      "  - as\n",
      "  - ne\n",
      "  - ▁ca\n",
      "  - lo\n",
      "  - ▁fa\n",
      "  - ▁him\n",
      "  - ng\n",
      "  - ▁good\n",
      "  - ▁could\n",
      "  - ▁pro\n",
      "  - ive\n",
      "  - ▁con\n",
      "  - de\n",
      "  - un\n",
      "  - age\n",
      "  - ▁ma\n",
      "  - '?'\n",
      "  - at\n",
      "  - ▁ro\n",
      "  - ▁ba\n",
      "  - ▁then\n",
      "  - ▁com\n",
      "  - est\n",
      "  - vi\n",
      "  - ▁dis\n",
      "  - ies\n",
      "  - ance\n",
      "  - ▁su\n",
      "  - ▁even\n",
      "  - ▁any\n",
      "  - ut\n",
      "  - ad\n",
      "  - ul\n",
      "  - ▁se\n",
      "  - ▁two\n",
      "  - ▁bu\n",
      "  - ▁lo\n",
      "  - ▁say\n",
      "  - ▁la\n",
      "  - ▁fi\n",
      "  - is\n",
      "  - ▁li\n",
      "  - ▁over\n",
      "  - ▁new\n",
      "  - ▁man\n",
      "  - ▁sp\n",
      "  - ity\n",
      "  - ▁did\n",
      "  - ▁bo\n",
      "  - ▁very\n",
      "  - x\n",
      "  - end\n",
      "  - ▁which\n",
      "  - ▁our\n",
      "  - ▁after\n",
      "  - ▁o\n",
      "  - ke\n",
      "  - ▁p\n",
      "  - im\n",
      "  - ▁want\n",
      "  - ▁ha\n",
      "  - ▁v\n",
      "  - z\n",
      "  - ▁where\n",
      "  - ard\n",
      "  - um\n",
      "  - ▁into\n",
      "  - ru\n",
      "  - ▁di\n",
      "  - ▁lot\n",
      "  - ▁dr\n",
      "  - mp\n",
      "  - ▁day\n",
      "  - ated\n",
      "  - ci\n",
      "  - ▁these\n",
      "  - ▁than\n",
      "  - ▁take\n",
      "  - ▁kind\n",
      "  - ▁got\n",
      "  - ight\n",
      "  - ▁make\n",
      "  - ence\n",
      "  - ▁pre\n",
      "  - ▁going\n",
      "  - ish\n",
      "  - ▁k\n",
      "  - able\n",
      "  - ▁look\n",
      "  - ti\n",
      "  - per\n",
      "  - ▁here\n",
      "  - ▁en\n",
      "  - ▁ah\n",
      "  - ry\n",
      "  - ▁too\n",
      "  - ▁part\n",
      "  - ant\n",
      "  - one\n",
      "  - ▁ho\n",
      "  - ▁much\n",
      "  - ▁way\n",
      "  - ▁sa\n",
      "  - ▁something\n",
      "  - mo\n",
      "  - ▁us\n",
      "  - ▁th\n",
      "  - ▁mhm\n",
      "  - ▁mi\n",
      "  - ▁off\n",
      "  - pe\n",
      "  - ▁back\n",
      "  - les\n",
      "  - ▁cr\n",
      "  - ▁ri\n",
      "  - ▁fe\n",
      "  - und\n",
      "  - ▁fl\n",
      "  - port\n",
      "  - ▁school\n",
      "  - ▁ch\n",
      "  - ▁should\n",
      "  - ▁first\n",
      "  - ▁only\n",
      "  - ▁le\n",
      "  - ot\n",
      "  - tion\n",
      "  - ▁little\n",
      "  - ▁da\n",
      "  - ▁hu\n",
      "  - ▁d\n",
      "  - me\n",
      "  - ta\n",
      "  - ▁down\n",
      "  - ▁okay\n",
      "  - ▁come\n",
      "  - ain\n",
      "  - ff\n",
      "  - ▁car\n",
      "  - co\n",
      "  - ▁need\n",
      "  - ture\n",
      "  - ▁many\n",
      "  - ▁things\n",
      "  - ▁ta\n",
      "  - qu\n",
      "  - man\n",
      "  - ty\n",
      "  - iv\n",
      "  - ▁year\n",
      "  - he\n",
      "  - ▁thing\n",
      "  - ho\n",
      "  - ▁singapore\n",
      "  - po\n",
      "  - ▁vi\n",
      "  - ▁sc\n",
      "  - ▁still\n",
      "  - der\n",
      "  - ▁hi\n",
      "  - ▁never\n",
      "  - ▁qu\n",
      "  - ia\n",
      "  - ▁fr\n",
      "  - ▁min\n",
      "  - ▁most\n",
      "  - om\n",
      "  - ful\n",
      "  - ▁bi\n",
      "  - ▁long\n",
      "  - ig\n",
      "  - ▁years\n",
      "  - ous\n",
      "  - ▁three\n",
      "  - ▁play\n",
      "  - ▁before\n",
      "  - ▁pi\n",
      "  - ical\n",
      "  - ▁those\n",
      "  - ▁comp\n",
      "  - huh\n",
      "  - ▁live\n",
      "  - tor\n",
      "  - ise\n",
      "  - ▁old\n",
      "  - am\n",
      "  - rr\n",
      "  - ▁sta\n",
      "  - ▁n\n",
      "  - ick\n",
      "  - di\n",
      "  - ma\n",
      "  - ary\n",
      "  - ction\n",
      "  - ▁friend\n",
      "  - ition\n",
      "  - ▁gu\n",
      "  - ▁through\n",
      "  - pp\n",
      "  - for\n",
      "  - ie\n",
      "  - ious\n",
      "  - ▁sh\n",
      "  - ▁home\n",
      "  - lu\n",
      "  - ▁high\n",
      "  - ian\n",
      "  - cu\n",
      "  - ▁help\n",
      "  - ▁give\n",
      "  - ▁talk\n",
      "  - ▁sha\n",
      "  - ▁such\n",
      "  - ▁didn\n",
      "  - em\n",
      "  - ▁may\n",
      "  - ▁ga\n",
      "  - ▁'\n",
      "  - ▁gra\n",
      "  - ▁guess\n",
      "  - ▁every\n",
      "  - ▁app\n",
      "  - tic\n",
      "  - ▁tra\n",
      "  - ▁\"\n",
      "  - op\n",
      "  - ▁made\n",
      "  - '\"'\n",
      "  - ▁op\n",
      "  - ▁own\n",
      "  - ▁mar\n",
      "  - 'no'\n",
      "  - ▁ph\n",
      "  - ▁life\n",
      "  - ▁y\n",
      "  - ak\n",
      "  - ine\n",
      "  - ▁pu\n",
      "  - ▁place\n",
      "  - ▁always\n",
      "  - ▁start\n",
      "  - ▁jo\n",
      "  - ▁pe\n",
      "  - ▁let\n",
      "  - ▁name\n",
      "  - ni\n",
      "  - ▁same\n",
      "  - ▁last\n",
      "  - ▁cl\n",
      "  - ph\n",
      "  - ▁both\n",
      "  - ▁pri\n",
      "  - ities\n",
      "  - ▁another\n",
      "  - and\n",
      "  - ▁al\n",
      "  - ▁boy\n",
      "  - ving\n",
      "  - ▁actually\n",
      "  - ▁person\n",
      "  - ▁went\n",
      "  - ▁yes\n",
      "  - ca\n",
      "  - ally\n",
      "  - ▁h\n",
      "  - ▁great\n",
      "  - ▁thought\n",
      "  - ▁used\n",
      "  - act\n",
      "  - ▁feel\n",
      "  - ward\n",
      "  - ▁different\n",
      "  - ▁cons\n",
      "  - ▁show\n",
      "  - ▁watch\n",
      "  - ▁being\n",
      "  - ▁money\n",
      "  - ay\n",
      "  - ▁try\n",
      "  - ▁why\n",
      "  - ▁big\n",
      "  - ens\n",
      "  - ▁cha\n",
      "  - ▁find\n",
      "  - ▁hand\n",
      "  - ▁real\n",
      "  - ▁four\n",
      "  - ial\n",
      "  - ▁ne\n",
      "  - ▁che\n",
      "  - ▁read\n",
      "  - ▁five\n",
      "  - ▁family\n",
      "  - ag\n",
      "  - ▁change\n",
      "  - ▁add\n",
      "  - ha\n",
      "  - ▁put\n",
      "  - par\n",
      "  - lic\n",
      "  - side\n",
      "  - ▁came\n",
      "  - ▁under\n",
      "  - ness\n",
      "  - ▁per\n",
      "  - j\n",
      "  - ▁around\n",
      "  - ▁end\n",
      "  - ▁house\n",
      "  - if\n",
      "  - ▁while\n",
      "  - vo\n",
      "  - ▁act\n",
      "  - ▁happen\n",
      "  - ▁plan\n",
      "  - mit\n",
      "  - ▁far\n",
      "  - ▁tri\n",
      "  - ▁ten\n",
      "  - ▁du\n",
      "  - ▁win\n",
      "  - ▁tea\n",
      "  - ze\n",
      "  - ▁better\n",
      "  - ▁sure\n",
      "  - ▁mu\n",
      "  - ▁use\n",
      "  - ▁anything\n",
      "  - ▁love\n",
      "  - ▁world\n",
      "  - ▁hard\n",
      "  - ure\n",
      "  - ▁does\n",
      "  - ▁war\n",
      "  - ▁stuff\n",
      "  - ▁ja\n",
      "  - ▁must\n",
      "  - min\n",
      "  - gg\n",
      "  - ▁ru\n",
      "  - ▁care\n",
      "  - ▁tell\n",
      "  - ▁pl\n",
      "  - ▁doing\n",
      "  - ▁probably\n",
      "  - ▁found\n",
      "  - ative\n",
      "  - ▁point\n",
      "  - ach\n",
      "  - ▁ju\n",
      "  - ip\n",
      "  - ▁again\n",
      "  - ▁interest\n",
      "  - ▁state\n",
      "  - ▁week\n",
      "  - na\n",
      "  - ▁might\n",
      "  - ▁pretty\n",
      "  - ▁ki\n",
      "  - ▁fo\n",
      "  - ber\n",
      "  - ▁am\n",
      "  - line\n",
      "  - led\n",
      "  - ▁six\n",
      "  - ▁acc\n",
      "  - ▁bri\n",
      "  - ▁call\n",
      "  - ▁sw\n",
      "  - ▁each\n",
      "  - ▁business\n",
      "  - ▁keep\n",
      "  - ▁away\n",
      "  - cause\n",
      "  - ▁pass\n",
      "  - ▁va\n",
      "  - ▁children\n",
      "  - ▁pay\n",
      "  - ▁count\n",
      "  - ▁public\n",
      "  - ▁everything\n",
      "  - land\n",
      "  - ▁though\n",
      "  - ▁men\n",
      "  - bo\n",
      "  - ▁young\n",
      "  - ▁na\n",
      "  - ▁move\n",
      "  - ough\n",
      "  - ating\n",
      "  - com\n",
      "  - ▁month\n",
      "  - ton\n",
      "  - ▁close\n",
      "  - ▁few\n",
      "  - '!'\n",
      "  - ▁maybe\n",
      "  - ▁imp\n",
      "  - son\n",
      "  - ▁grow\n",
      "  - ▁u\n",
      "  - ▁turn\n",
      "  - ible\n",
      "  - ▁em\n",
      "  - ▁air\n",
      "  - ▁ever\n",
      "  - our\n",
      "  - ▁sea\n",
      "  - ▁fun\n",
      "  - ▁government\n",
      "  - ▁miss\n",
      "  - ▁done\n",
      "  - ▁next\n",
      "  - ▁kids\n",
      "  - ▁cor\n",
      "  - ▁set\n",
      "  - ▁run\n",
      "  - way\n",
      "  - ▁wa\n",
      "  - ▁getting\n",
      "  - ▁eight\n",
      "  - ▁open\n",
      "  - ▁job\n",
      "  - ▁problem\n",
      "  - ook\n",
      "  - ▁night\n",
      "  - ▁learn\n",
      "  - ▁book\n",
      "  - ual\n",
      "  - ▁ti\n",
      "  - ▁best\n",
      "  - cept\n",
      "  - ▁during\n",
      "  - ▁small\n",
      "  - ex\n",
      "  - ▁without\n",
      "  - ▁water\n",
      "  - ▁trans\n",
      "  - ▁course\n",
      "  - ▁once\n",
      "  - ▁sit\n",
      "  - ▁area\n",
      "  - ▁country\n",
      "  - ▁mister\n",
      "  - ▁nothing\n",
      "  - ▁whole\n",
      "  - ▁believe\n",
      "  - ▁service\n",
      "  - ▁took\n",
      "  - ▁face\n",
      "  - ▁bad\n",
      "  - ▁later\n",
      "  - ▁head\n",
      "  - ▁called\n",
      "  - ▁seven\n",
      "  - ▁art\n",
      "  - ▁since\n",
      "  - ▁er\n",
      "  - ▁fact\n",
      "  - ▁city\n",
      "  - ▁market\n",
      "  - ▁hour\n",
      "  - ▁continue\n",
      "  - ship\n",
      "  - ▁invest\n",
      "  - ▁exactly\n",
      "  - ▁large\n",
      "  - ▁true\n",
      "  - ▁nine\n",
      "  - ▁sub\n",
      "  - ▁having\n",
      "  - ▁game\n",
      "  - va\n",
      "  - ▁lu\n",
      "  - ▁conf\n",
      "  - ▁case\n",
      "  - ▁doesn\n",
      "  - ▁certain\n",
      "  - ▁wi\n",
      "  - ▁law\n",
      "  - ▁else\n",
      "  - fi\n",
      "  - ▁left\n",
      "  - ▁enough\n",
      "  - ▁second\n",
      "  - ▁gonna\n",
      "  - ▁food\n",
      "  - ▁hope\n",
      "  - ▁saw\n",
      "  - ▁between\n",
      "  - ▁je\n",
      "  - bi\n",
      "  - ▁girl\n",
      "  - ▁company\n",
      "  - ▁able\n",
      "  - ▁expect\n",
      "  - ▁told\n",
      "  - ▁stand\n",
      "  - ▁group\n",
      "  - ▁main\n",
      "  - ▁walk\n",
      "  - ▁cause\n",
      "  - ▁however\n",
      "  - ▁number\n",
      "  - ▁follow\n",
      "  - ▁near\n",
      "  - ▁yet\n",
      "  - ▁sometimes\n",
      "  - ▁train\n",
      "  - ▁lead\n",
      "  - ▁system\n",
      "  - ▁remain\n",
      "  - ▁develop\n",
      "  - gra\n",
      "  - ▁word\n",
      "  - ▁exc\n",
      "  - ▁together\n",
      "  - ▁consider\n",
      "  - ▁town\n",
      "  - ▁less\n",
      "  - ator\n",
      "  - ▁important\n",
      "  - ▁remember\n",
      "  - ▁free\n",
      "  - ▁quite\n",
      "  - ▁understand\n",
      "  - ▁bra\n",
      "  - ▁support\n",
      "  - ▁idea\n",
      "  - ▁stop\n",
      "  - ▁reason\n",
      "  - ▁nice\n",
      "  - ▁mm\n",
      "  - ▁agree\n",
      "  - ▁low\n",
      "  - ▁against\n",
      "  - ▁issue\n",
      "  - ▁become\n",
      "  - ▁today\n",
      "  - ▁side\n",
      "  - ▁student\n",
      "  - ▁matter\n",
      "  - ▁question\n",
      "  - ▁mother\n",
      "  - ▁father\n",
      "  - ▁hundred\n",
      "  - ▁sort\n",
      "  - ▁eat\n",
      "  - ▁already\n",
      "  - ▁rest\n",
      "  - ▁line\n",
      "  - ▁asked\n",
      "  - ▁include\n",
      "  - ▁upon\n",
      "  - ▁office\n",
      "  - ▁won\n",
      "  - ▁class\n",
      "  - ▁wait\n",
      "  - ▁twenty\n",
      "  - ▁half\n",
      "  - ▁light\n",
      "  - ▁price\n",
      "  - ▁almost\n",
      "  - ash\n",
      "  - ▁child\n",
      "  - ▁sign\n",
      "  - ▁least\n",
      "  - ▁several\n",
      "  - press\n",
      "  - ▁either\n",
      "  - ▁minute\n",
      "  - ▁himself\n",
      "  - ▁parents\n",
      "  - ▁room\n",
      "  - ▁whatever\n",
      "  - ▁general\n",
      "  - ▁cost\n",
      "  - ▁among\n",
      "  - ▁direct\n",
      "  - ▁computer\n",
      "  - ▁appear\n",
      "  - ▁meet\n",
      "  - ▁ski\n",
      "  - ▁return\n",
      "  - ▁couple\n",
      "  - ▁product\n",
      "  - ▁suppose\n",
      "  - ▁definitely\n",
      "  - ▁america\n",
      "  - ▁term\n",
      "  - ▁usually\n",
      "  - ▁strong\n",
      "  - ▁current\n",
      "  - ▁arm\n",
      "  - ▁speak\n",
      "  - ▁local\n",
      "  - ▁south\n",
      "  - ▁experience\n",
      "  - ▁full\n",
      "  - ▁north\n",
      "  - ▁elect\n",
      "  - ▁leave\n",
      "  - ▁provide\n",
      "  - qui\n",
      "  - ▁power\n",
      "  - ▁movie\n",
      "  - ▁everyone\n",
      "  - ▁making\n",
      "  - ▁member\n",
      "  - ▁woman\n",
      "  - ▁somebody\n",
      "  - ▁wonder\n",
      "  - ▁short\n",
      "  - ▁health\n",
      "  - ▁police\n",
      "  - ▁bank\n",
      "  - ▁until\n",
      "  - ▁companies\n",
      "  - ▁everybody\n",
      "  - ▁knew\n",
      "  - ▁program\n",
      "  - ▁music\n",
      "  - ▁york\n",
      "  - ▁land\n",
      "  - ▁doctor\n",
      "  - ▁answer\n",
      "  - ▁building\n",
      "  - ▁employ\n",
      "  - ▁travel\n",
      "  - ▁major\n",
      "  - ▁seems\n",
      "  - ▁safe\n",
      "  - gue\n",
      "  - ▁college\n",
      "  - ▁along\n",
      "  - ▁clear\n",
      "  - ▁especially\n",
      "  - ▁umhu\n",
      "  - ▁result\n",
      "  - ▁type\n",
      "  - ▁court\n",
      "  - ▁black\n",
      "  - ▁hold\n",
      "  - ▁myself\n",
      "  - ▁education\n",
      "  - ▁social\n",
      "  - ▁enjoy\n",
      "  - ▁became\n",
      "  - ▁whether\n",
      "  - ▁morning\n",
      "  - ▁difficult\n",
      "  - ▁shi\n",
      "  - ▁felt\n",
      "  - ▁husband\n",
      "  - ▁white\n",
      "  - ▁taking\n",
      "  - ▁million\n",
      "  - ▁require\n",
      "  - ▁early\n",
      "  - ency\n",
      "  - ▁visit\n",
      "  - ▁level\n",
      "  - ▁brother\n",
      "  - ▁married\n",
      "  - ▁further\n",
      "  - ▁affect\n",
      "  - ▁serve\n",
      "  - ▁present\n",
      "  - ▁park\n",
      "  - ▁effect\n",
      "  - ▁wife\n",
      "  - ▁teacher\n",
      "  - ▁cannot\n",
      "  - ▁community\n",
      "  - ▁street\n",
      "  - ▁period\n",
      "  - ▁national\n",
      "  - ▁view\n",
      "  - ▁future\n",
      "  - ▁daughter\n",
      "  - ▁situation\n",
      "  - ▁grand\n",
      "  - ▁success\n",
      "  - ▁perform\n",
      "  - ▁concern\n",
      "  - ▁complete\n",
      "  - ▁example\n",
      "  - ized\n",
      "  - ▁thousand\n",
      "  - ▁increase\n",
      "  - ▁began\n",
      "  - ▁final\n",
      "  - ▁east\n",
      "  - ▁sense\n",
      "  - ▁charge\n",
      "  - ▁record\n",
      "  - ▁born\n",
      "  - ▁instead\n",
      "  - ▁receive\n",
      "  - ▁women\n",
      "  - ▁across\n",
      "  - ▁information\n",
      "  - ▁although\n",
      "  - ▁process\n",
      "  - ▁condition\n",
      "  - ▁security\n",
      "  - ▁treat\n",
      "  - ▁funny\n",
      "  - ▁custom\n",
      "  - ▁cold\n",
      "  - ▁behind\n",
      "  - ified\n",
      "  - ▁ground\n",
      "  - cycl\n",
      "  - ▁depend\n",
      "  - ▁themselves\n",
      "  - ▁design\n",
      "  - ▁slow\n",
      "  - ▁third\n",
      "  - ▁smoke\n",
      "  - ▁wrong\n",
      "  - ▁project\n",
      "  - ▁space\n",
      "  - ▁drink\n",
      "  - ▁particular\n",
      "  - ▁listen\n",
      "  - ▁thirty\n",
      "  - ▁special\n",
      "  - ability\n",
      "  - ▁improve\n",
      "  - ▁attack\n",
      "  - ▁happy\n",
      "  - ▁strange\n",
      "  - ▁english\n",
      "  - ▁value\n",
      "  - ▁brought\n",
      "  - ▁private\n",
      "  - ▁account\n",
      "  - ▁china\n",
      "  - ▁spoke\n",
      "  - ▁foreign\n",
      "  - ▁possible\n",
      "  - ▁author\n",
      "  - ▁circ\n",
      "  - ▁voice\n",
      "  - ▁figure\n",
      "  - ▁control\n",
      "  - ▁according\n",
      "  - ▁green\n",
      "  - ▁university\n",
      "  - ▁language\n",
      "  - ▁please\n",
      "  - ▁animal\n",
      "  - ▁church\n",
      "  - ▁society\n",
      "  - ▁dream\n",
      "  - ’\n",
      "  - q\n",
      "  - ':'\n",
      "  - ;\n",
      "  - —\n",
      "  - ‘\n",
      "  - ”\n",
      "  - _\n",
      "  - '3'\n",
      "  - '8'\n",
      "  - <\n",
      "  - '>'\n",
      "  - '1'\n",
      "  - –\n",
      "  - '7'\n",
      "  - (\n",
      "  - )\n",
      "  - '0'\n",
      "  - '2'\n",
      "  - '4'\n",
      "  - +\n",
      "  - '&'\n",
      "  - '5'\n",
      "  - '9'\n",
      "  - ü\n",
      "  - é\n",
      "  - /\n",
      "  - á\n",
      "  - ó\n",
      "  - ō\n",
      "  - ú\n",
      "  - ']'\n",
      "  - â\n",
      "  - í\n",
      "  - ã\n",
      "  - ð\n",
      "  - ā\n",
      "  - ć\n",
      "  - č\n",
      "  - š\n",
      "  - è\n",
      "  - ë\n",
      "  - '`'\n",
      "  - ç\n",
      "  - ū\n",
      "  - ạ\n",
      "  - ø\n",
      "  - '='\n",
      "  - à\n",
      "  - ł\n",
      "  - α\n",
      "  - ô\n",
      "  - к\n",
      "  - '}'\n",
      "  - å\n",
      "  - ă\n",
      "  - и\n",
      "  - ī\n",
      "  - π\n",
      "  - œ\n",
      "  - \\\n",
      "  - '['\n",
      "  - ñ\n",
      "  - ß\n",
      "  - ö\n",
      "  - ä\n",
      "  - '6'\n",
      "  - з\n",
      "  - н\n",
      "  - û\n",
      "  - '%'\n",
      "  - '{'\n",
      "  - ¡\n",
      "  - æ\n",
      "  - ê\n",
      "  - þ\n",
      "  - ę\n",
      "  - ě\n",
      "  - ğ\n",
      "  - ń\n",
      "  - ő\n",
      "  - ř\n",
      "  - ž\n",
      "  - ʻ\n",
      "  - в\n",
      "  - е\n",
      "  - й\n",
      "  - л\n",
      "  - ь\n",
      "  - χ\n",
      "  - “\n",
      "optim:\n",
      "  name: novograd\n",
      "  lr: 0.05\n",
      "  betas:\n",
      "  - 0.8\n",
      "  - 0.25\n",
      "  weight_decay: 0.001\n",
      "  sched:\n",
      "    name: CosineAnnealing\n",
      "    warmup_steps: 1000\n",
      "    warmup_ratio: null\n",
      "    min_lr: 1.0e-09\n",
      "    last_epoch: -1\n",
      "target: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE\n",
      "nemo_version: 1.16.0\n",
      "decoding:\n",
      "  strategy: greedy\n",
      "  preserve_alignments: null\n",
      "  compute_timestamps: null\n",
      "  word_seperator: ' '\n",
      "  ctc_timestamp_type: all\n",
      "  batch_dim_index: 0\n",
      "  greedy:\n",
      "    preserve_alignments: false\n",
      "    compute_timestamps: false\n",
      "    preserve_frame_confidence: false\n",
      "    confidence_method_cfg: null\n",
      "  beam:\n",
      "    beam_size: 4\n",
      "    search_type: default\n",
      "    preserve_alignments: false\n",
      "    compute_timestamps: false\n",
      "    return_best_hypothesis: true\n",
      "    beam_alpha: 1.0\n",
      "    beam_beta: 0.0\n",
      "    kenlm_path: null\n",
      "    flashlight_cfg:\n",
      "      lexicon_path: null\n",
      "      beam_size_token: 16\n",
      "      beam_threshold: 20.0\n",
      "      unk_weight: -.inf\n",
      "      sil_weight: 0.0\n",
      "      unit_lm: false\n",
      "  confidence_cfg:\n",
      "    preserve_frame_confidence: false\n",
      "    preserve_token_confidence: false\n",
      "    preserve_word_confidence: false\n",
      "    exclude_blank: true\n",
      "    aggregation: min\n",
      "    method_cfg:\n",
      "      name: entropy\n",
      "      entropy_type: tsallis\n",
      "      temperature: 0.33\n",
      "      entropy_norm: exp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = copy.deepcopy(citrinet.cfg)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the contents of the Model config\n",
    "---\n",
    "\n",
    "NeMo models contain the entire definition of the neural network as well as most of the surrounding infrastructure to support that model within themselves\n",
    "\n",
    "Citrinet contains within its config\n",
    "- `preprocessor` - MelSpectrogram preprocessing layer\n",
    "- `encoder` - The acoustic encoder model\n",
    "- `decoder` - The CTC decoder model\n",
    "- `optim` (and potientally `sched`) - Optimizer configuration, Can optionally include Scheduler info\n",
    "- `spec_augment` - Spectrogram Augmentation support\n",
    "- `train_ds`,`validation_ds`, and `test_ds` - Dataset and data loader construction information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the contents of the Model config\n",
    "---\n",
    "Say we want to experiment with a different preprocessor or we want to add a scheduler to this model during training. \\\n",
    "OmegaConf makes this a very simple task for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old config: \n",
      "name: novograd\n",
      "lr: 0.05\n",
      "betas:\n",
      "- 0.8\n",
      "- 0.25\n",
      "weight_decay: 0.001\n",
      "sched:\n",
      "  name: CosineAnnealing\n",
      "  warmup_steps: 1000\n",
      "  warmup_ratio: null\n",
      "  min_lr: 1.0e-09\n",
      "  last_epoch: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OmegaConf won't allow to add new config items, so we temporarily disable this safeguard\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "# Let's see the old optim config \n",
    "print(\"Old config: \")\n",
    "print(OmegaConf.to_yaml(cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Config: \n",
      "name: novograd\n",
      "lr: 0.05\n",
      "betas:\n",
      "- 0.8\n",
      "- 0.25\n",
      "weight_decay: 0.001\n",
      "sched:\n",
      "  name: CosineAnnealing\n",
      "  warmup_steps: 1000\n",
      "  min_lr: 1.0e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sched = {'name': 'CosineAnnealing', 'warmup_steps': 1000, 'min_lr': 1e-6}\n",
    "sched = OmegaConf.create(sched)\n",
    "\n",
    "# Assign it to cfg.optim.sched namespace\n",
    "cfg.optim.sched = sched\n",
    "\n",
    "# Let's see the new optim config\n",
    "print(\"New Config: \")\n",
    "print(OmegaConf.to_yaml(cfg.optim))\n",
    "\n",
    "# Here, we restore the safeguards so no more additions can be made to the config\n",
    "OmegaConf.set_struct(cfg, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the model from the config\n",
    "---\n",
    "NeMo models can be updated in a few ways, but we follow similar patterns within each collection so as to maintain consistency\n",
    "\n",
    "Here, we will show the 2 most common ways to modify core components of the model - using the `from_config_dict` method, and updating a few special parts of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update model using `from_config_dict`\n",
    "\n",
    "In cetain config filesm you will notice the following pattern:\n",
    "\n",
    "```yaml\n",
    "preprocessor:\n",
    "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
    "    normalize: per_feature\n",
    "    window_size: 0.02\n",
    "    sample_rate: 16000\n",
    "    window_stride: 0.01\n",
    "    window: hann\n",
    "    features: 64\n",
    "    n_fft: 512\n",
    "    frame_splicing: 1\n",
    "    dither: 1.0e-05\n",
    "    stft_conv: false\n",
    "```\n",
    "\n",
    "You might ask why we are using `_target_`? Well, it is generally rare for the preprocessor, encoder, decodere and perhaps a few other details to be changed often from the command line when experimenting. In order to stabilize these settings, we enforce that our preprocessor will always be of type `AudioToMelSpectrogramPreprocessor` for this model by setting its `_target_` attribute in the config. In order to provide its parameters in the class constructor, we simply add them after `_target_`.\n",
    "\n",
    "\n",
    "---\n",
    "Note we can still change all of the parameters of this `AudioMelSpectrogramPreprocessor` class from the CLI using hydra, so we don't lose any flexibility once we decide what type of preprocessing class we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-19 16:14:37 features:286] PADDING: 16\n",
      "AudioToMelSpectrogramPreprocessor(\n",
      "  (featurizer): FilterbankFeatures()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "new_preprocessor_config = copy.deepcopy(cfg.preprocessor)\n",
    "new_preprocessor = citrinet.from_config_dict(new_preprocessor_config)\n",
    "print(new_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name              | Type                              | Params\n",
       "------------------------------------------------------------------------\n",
       "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
       "1 | encoder           | ConvASREncoder                    | 36.3 M\n",
       "2 | decoder           | ConvASRDecoder                    | 657 K \n",
       "3 | loss              | CTCLoss                           | 0     \n",
       "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
       "5 | _wer              | WERBPE                            | 0     \n",
       "------------------------------------------------------------------------\n",
       "37.0 M    Trainable params\n",
       "0         Non-trainable params\n",
       "37.0 M    Total params\n",
       "147.977   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citrinet.preprocessor = new_preprocessor\n",
    "citrinet.summarize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserving the new config\n",
    "\n",
    "We need to perform a crucial step - **preserving the updated config**\n",
    "NeMo has may ways of saving and restoring its models. All of them depend on having an updated config that defines the model in its entirety, so if we modify anything, we should also update the corresponding part of the config to safely save and restore models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the config copy\n",
    "cfg.preprocessor = new_preprocessor_config\n",
    "# update the model config\n",
    "citrinet.cfg = cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update a few special components of the Model\n",
    "\n",
    "---\n",
    "\n",
    "While the above approach is good for most major components of the model, Nemo has special utilities for a few components. They are\n",
    "\n",
    "    - `setup_training_data`\n",
    "    - `setup_validation_data` and `setup_multi_validation_data`\n",
    "    - `setup_test_data` and `setup_multi_test_data`\n",
    "    - `setup_optimization`\n",
    "\n",
    "One if the major tasks of all conversational AI models is fine-tuning onto new datasets - new languages, new corpus of text, new voices etc. It is often insufficient th have just a pre-trained model. So these setup methods are provided to enable users to adapt models after they have been already trained or provided to you\n",
    "\n",
    "Let's discuss how to add the scheduler to the model below (which initially had just an optimizer in its config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: novograd\n",
      "lr: 0.05\n",
      "betas:\n",
      "- 0.8\n",
      "- 0.25\n",
      "weight_decay: 0.001\n",
      "sched:\n",
      "  name: CosineAnnealing\n",
      "  warmup_steps: 1000\n",
      "  warmup_ratio: null\n",
      "  min_lr: 1.0e-09\n",
      "  last_epoch: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the current optimizer\n",
    "print(OmegaConf.to_yaml(citrinet.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's update the config\n",
    "citrinet.setup_optimization(cfg.optim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
