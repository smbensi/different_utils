{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a NeMo model\n",
    "\n",
    "NeMo \"Models\" are comprised of a few key components . We'll go in the order (NN arch, Dataset + Data Loaders, Preprocessing + Postprocessing, Optimizer + scheduler , other infrastructure: tokenizers, LM configs, data augmentation, etc)\n",
    "\n",
    "To make this slightly challenging, let's port a model from the NLP domain. Transformers are all the rage, with BERT and his friends from Sesame street forming the core infrastructure for many NLP tasks.\n",
    "\n",
    "An excellent implementation of one such model - GPT - can be found in the `minGPT` repo https://github.com/karpathy/minGPT. We will attempt to port minGPT to NeMo "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction the Neural Network Architecture\n",
    "\n",
    "First, on the list - the neural network that forms the backbone of the NeMo model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-22 10:48:37 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nemo\n",
    "from nemo.core import NeuralModule\n",
    "from nemo.core import typecheck"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NeuralModule` is a subclass of `torch.nn.Module` and it brings with it a few additional functionalities \n",
    "It has the following capabilities:\n",
    "1. `Typing` it add supports for `Neural Type Checking` to the model. `Typing` is optional but quite useful\n",
    "2. `Serialization` the `OmegaConf` config dict and YAML config files . all `NeuralModules` inherhently supports serialization/deserialization from such config dict\n",
    "3. `FileIO` optional file serialization system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmptyModule(NeuralModule):\n",
    "\n",
    "    def forward(self):\n",
    "        print(\"Neural Module - hello world\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Module - hello world\n"
     ]
    }
   ],
   "source": [
    "x = MyEmptyModule()\n",
    "x()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural types\n",
    "\n",
    "Almost all NeMo components inherit the classs `Typing`. `Typing` is a simple class that adds 2 properties to the class that inherits it - `input_type` and `output_types`. A NeuralType is simply a semantic tensor. It contains info regarding the semantic shape and the tensor should hold, as well as the semantic info of what the tensor represents. \n",
    "\n",
    "So what semantic info does such a typed tensor contain? \n",
    "\n",
    "Across the DL domain, we often encounter cases where tensor shapes may match, but the semantics don't match at all. For example let's take a look at the following rank 3 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[0, 8, 9, 0, 3]])\n",
      "embedding(x) : torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=30)\n",
    "x = torch.randint(high=10, size=(1,5))\n",
    "print(\"x: \", x)\n",
    "print(\"embedding(x) :\", embedding(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[ 0.8871],\n",
      "         [-0.1232],\n",
      "         [-0.7368],\n",
      "         [-2.8013],\n",
      "         [ 0.5705]]])\n",
      "lstm(x) : torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "# Case 2\n",
    "lstm = torch.nn.LSTM(1, 30, batch_first=True)\n",
    "x = torch.randn(1, 5, 1)\n",
    "print(\"x: \", x)\n",
    "print(\"lstm(x) :\", lstm(x)[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to recognize that the 2 tensors do not represent the same semantic info is precisely why we utilize Neural Types. It contains the info of both the shape and the semantic concept of what that tensor represents. If we performed a neural type check between the 2 outputs of those tensors, it would raise an error saying semantically they were different things (they are `INCOMPATIBLE` with each other)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Types - Usage\n",
    "\n",
    "Neural Types are one of the core foundations of NeMo. While they are entirely *optional* and not instrusive, NeMo takes great care to support it so that there is no semantic incompatibility between components being used by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core.neural_types import NeuralType\n",
    "from nemo.core.neural_types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModule(NeuralModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=30)\n",
    "    \n",
    "    @typecheck()\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    @property\n",
    "    def input_types(self):\n",
    "        return {\n",
    "            'x': NeuralType(axes=('B','T'), elements_type=Index())\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def output_types(self):\n",
    "        return {\n",
    "            'y': NeuralType(axes=('B','T','C'), elements_type=EmbeddedTextType())\n",
    "        }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss how we added  type checking support to the above class\n",
    "1. `forward` has a decorator `@typecheck()` on it\n",
    "2. `input_types` and `output_types` properties are defined\n",
    "\n",
    "Let's expand on each of the above steps\n",
    "- `@typecheck()` is a simple decorator that takes any class that inherits `Typing` (NeuralModule does this for us) and adds the 2 default properties of `input_types` and `output_types`, which by default return None\n",
    "\n",
    "The `@typecheck()` decorator's explicit use ensures that, by default, neural type checking is **disabled**. NeMo does not wish to intrude on the development process of models. So users can 'opt-in' to type checking by overriding the 2 properties. Therefore, the decorator ensures that users are not burdened with type checking before they wish to have it.\n",
    "\n",
    "So what is `@typecheck()`? Simply put, you can wrap **any** function of a class that inherits `Typing` with this decorator, and it will look up the definition of the types of that class and enforce them. Typically,, `torch.nn.Module` subclasses only implement `forward()` so it is most common to wrap that method, but `@typecheck()` is a very flexible decorator. \n",
    "\n",
    "As we see above, `@typecheck()` enforces the types. How then, do we provide this type of info to NeMo ?\n",
    "\n",
    "By overriding `input_types` and `output_types` properties of the class, we can return a dictionary mapping a string name to a `NeuralType`. In the above case, we define a `NeuralType` as 2 components.\n",
    "- `axes`: this is the semantic info of the carried by the axes themselves. The most common axes info is from single character notation.\\\n",
    "  `B` = batch\\\n",
    "  `C` / `D` - Channel/Dimension (treated the same)\\\n",
    "  `T` - Time\\\n",
    "  `H` / `W` : Height/Width\n",
    "- `element_types`: This is the semantic info of \"what the tensor represents\". All such types are derived from the basic `ElementType`, and merely subclassing `ElementType` allows us tobuild a hierarchy of custom semantic types that can be used by NeMo\n",
    "\n",
    "Here we declare that the input is an element_type of `Index` (index of the character in the vocabulary) and that the output is an element_type of `EmbeddedTextType` (the text embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_module = EmbeddingModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class LSTMModule(NeuralModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(1,30,batch_first=True)\n",
    "\n",
    "    @typecheck()\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)\n",
    "\n",
    "    @property\n",
    "    def input_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            'x': NeuralType(axes=('B', 'T', 'C'), elements_type=SpectrogramType())\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            'y': NeuralType(axes=('B', 'T', 'C'), elements_type=EncodedRepresentation())\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the input to be a rank 3 tensor, now representing a 'SpectrogramType', we intentionally keep it generic - it can be a `MelSpectrogramType` or a `MFCCSpectrogramType` as its input !\n",
    "\n",
    "The output of a LSTM is now an `EncodedRepresentation`. Pratically, this can be the output of a CNN layer, a Transformer block, or in this case, a LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_module = LSTMModule()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[3, 9, 3, 2, 9]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "All arguments must be passed by kwargs only for typed methods",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(high\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx:\u001b[39m\u001b[39m\"\u001b[39m,x1)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39membedding(x):\u001b[39m\u001b[39m\"\u001b[39m,embedding_module(x1)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/nemo/core/classes/common.py:1081\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# Check that all arguments are kwargs\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m input_types \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arguments must be passed by kwargs only for typed methods\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Perform rudimentary input checks here\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m instance\u001b[39m.\u001b[39m_validate_input_types(input_types\u001b[39m=\u001b[39minput_types, ignore_collections\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_collections, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: All arguments must be passed by kwargs only for typed methods"
     ]
    }
   ],
   "source": [
    "# Case 1 [ERROR Cell]\n",
    "x1 = torch.randint(high=10, size=(1,5))\n",
    "print(\"x:\",x1)\n",
    "print(\"embedding(x):\",embedding_module(x1).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why we get a `TypeError` right off the bat. This `TypeError` is raised by design.\n",
    "\n",
    "Positional arguments can cause significant issues during model development, mostly when the model/module is not finalized. To reduce the potential for mistakes cause by wrong\n",
    "positional arguments and enforce the name of arguments provided to the function, `Typing` requires you to **call of your type-checked functions by kwargs only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[3, 9, 3, 2, 9]])\n",
      "embedding(x):  torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "print(\"x:\", x1)\n",
    "print(\"embedding(x): \",embedding_module(x=x1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[-0.2252],\n",
      "         [ 0.7333],\n",
      "         [-1.1834],\n",
      "         [ 1.6252],\n",
      "         [ 2.0465]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Number of output arguments provided (2) is not as expected. It should be larger or equal than 1 and less or equal than 1.\nThis can be either because insufficient/extra number of output NeuralTypes were provided,or the provided NeuralTypes {'y': NeuralType(axis=(batch, time, dimension), element_type=EncodedRepresentation)} should enable container support (add '[]' to the NeuralType definition)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx:\u001b[39m\u001b[39m\"\u001b[39m, x2)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlstm(x): \u001b[39m\u001b[39m\"\u001b[39m,lstm_module(x\u001b[39m=\u001b[39;49mx2)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/nemo/core/classes/common.py:1089\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[39m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m outputs \u001b[39m=\u001b[39m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1089\u001b[0m instance\u001b[39m.\u001b[39;49m_attach_and_validate_output_types(\n\u001b[1;32m   1090\u001b[0m     output_types\u001b[39m=\u001b[39;49moutput_types, ignore_collections\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_collections, out_objects\u001b[39m=\u001b[39;49moutputs\n\u001b[1;32m   1091\u001b[0m )\n\u001b[1;32m   1093\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/nemo/core/classes/common.py:290\u001b[0m, in \u001b[0;36mTyping._attach_and_validate_output_types\u001b[0;34m(self, out_objects, ignore_collections, output_types)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m# In all other cases, python will wrap multiple outputs into an outer tuple.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# Allow number of output arguments to be <= total output neural types and >= mandatory outputs.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out_container) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(out_types_list) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(out_container) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(mandatory_out_types_list):\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of output arguments provided (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is not as expected. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt should be larger or equal than \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and less or equal than \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis can be either because insufficient/extra number of output NeuralTypes were provided,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor the provided NeuralTypes \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m should enable container support \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(add \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to the NeuralType definition)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    296\u001b[0m             \u001b[39mlen\u001b[39m(out_container), \u001b[39mlen\u001b[39m(out_types_list), \u001b[39mlen\u001b[39m(mandatory_out_types_list), output_types\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    300\u001b[0m \u001b[39m# Attach types recursively, if possible\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(out_objects, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(out_objects, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    302\u001b[0m     \u001b[39m# Here, out_objects is a single object which can potentially be attached with a NeuralType\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Number of output arguments provided (2) is not as expected. It should be larger or equal than 1 and less or equal than 1.\nThis can be either because insufficient/extra number of output NeuralTypes were provided,or the provided NeuralTypes {'y': NeuralType(axis=(batch, time, dimension), element_type=EncodedRepresentation)} should enable container support (add '[]' to the NeuralType definition)"
     ]
    }
   ],
   "source": [
    "# Case 2: ERROR Cell\n",
    "x2 = torch.randn(1, 5, 1)\n",
    "print(\"x:\", x2)\n",
    "print(\"lstm(x): \",lstm_module(x=x2).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our `LSTMModule` class, we declare the output types to be a single NeuralType- an `EncodedRepresentation` of shape [B,T,C]\n",
    "\n",
    "But the output of a LSTM layer is a tuple of 1) the encoded representation of shape [B,T,C] 2) another tuple containing 2 state values - the hidden state `h` and the\n",
    "cell state `c`, each of shape [num_layers*num_directions, B, C]!\n",
    "\n",
    "So the neural type system raises an error saying that the number of output arguments does not match what is expected\n",
    "\n",
    "Let's fix the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectLSTMModule(LSTMModule):\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            'y': NeuralType(axes=('B', 'T', 'C'), elements_type=EncodedRepresentation()),\n",
    "            'h_c': [NeuralType(axes=('D', 'B', 'C'), elements_type=EncodedRepresentation())]\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note that for the `h_c` neural type, we wrap it in a list `[]`. NeMo, by default, assumes that each `NeuralType` corresponds to a single returned vvalue.\n",
    "However, in the case of LSTMs, they produce a tuple of 2-state tensors.\n",
    "\n",
    "So we inform NeMo that this particular `NeuralType` is a single dimensional list of items- and that each element of this list shares the same `NeuralType` and has the\n",
    "same shape.\n",
    "\n",
    "NeMo then ensures that the `h_c` is always a list of tensors. It will not check how many items are in the list, but will ensure that the returned value msut be a list containing zero or more items - and that each of these items share the same `NeuralType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_module = CorrectLSTMModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[-1.8756],\n",
      "         [-0.7585],\n",
      "         [ 1.0798],\n",
      "         [ 0.8792],\n",
      "         [-0.3083]]])\n",
      "lstm(x):  torch.Size([1, 5, 30])\n",
      "hidden_state (h): torch.Size([1, 1, 30])\n",
      "hidden_state (c): torch.Size([1, 1, 30])\n"
     ]
    }
   ],
   "source": [
    "# Case 2\n",
    "x2 = torch.randn(1,5,1)\n",
    "y2, (h, c) = lstm_module(x=x2)\n",
    "print(\"x: \",x2)\n",
    "print(\"lstm(x): \", y2.shape) # The output of the LSTM RNN\n",
    "print(\"hidden_state (h):\", h.shape) #The 1st hidden state of the LSTM RNN\n",
    "print(\"hidden_state (c):\", c.shape) #The 2nd hidden state of the LSTM RNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `output_types` is overridden, and valid torch tensors are returned as a result, these tensors are attached with the attribute `neural_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_out = embedding_module(x=x1)\n",
    "lstm_out = lstm_module(x=x2)[0]\n",
    "\n",
    "assert hasattr(emb_out, 'neural_type')\n",
    "assert hasattr(lstm_out, 'neural_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding tensor :  axes: (batch, time, dimension); elements_type: EmbeddedTextType\n",
      "LSTM tensor :  axes: (batch, time, dimension); elements_type: EncodedRepresentation\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding tensor : \", emb_out.neural_type)\n",
    "print(\"LSTM tensor : \", lstm_out.neural_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NeuralTypeComparisonResult.INCOMPATIBLE: 6>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_out.neural_type.compare(lstm_out.neural_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NeuralTypeComparisonResult.INCOMPATIBLE: 6>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_out.neural_type == lstm_out.neural_type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Types - Limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding(x):  torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "embedding_module = EmbeddingModule()\n",
    "x1 = torch.randint(high=10, size=(1,5))\n",
    "\n",
    "# Attach correct neural type\n",
    "x1.neural_type =  NeuralType(('B','T'), Index())\n",
    "\n",
    "print(\"embedding(x): \", embedding_module(x=x1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NeuralTypeComparisonResult.INCOMPATIBLE :\nInput type expected : axes: (batch, time); elements_type: Index\nInput type found : axes: (batch, time); elements_type: LabelsType\nArgument: x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Attach wrong neural type [ERROR CELL]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x1\u001b[39m.\u001b[39mneural_type \u001b[39m=\u001b[39m NeuralType((\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m), LabelsType())\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39membedding(x): \u001b[39m\u001b[39m\"\u001b[39m, embedding_module(x\u001b[39m=\u001b[39;49mx1)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/nemo/core/classes/common.py:1084\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arguments must be passed by kwargs only for typed methods\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Perform rudimentary input checks here\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m instance\u001b[39m.\u001b[39;49m_validate_input_types(input_types\u001b[39m=\u001b[39;49minput_types, ignore_collections\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_collections, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1086\u001b[0m \u001b[39m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m outputs \u001b[39m=\u001b[39m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.9/site-packages/nemo/core/classes/common.py:219\u001b[0m, in \u001b[0;36mTyping._validate_input_types\u001b[0;34m(self, input_types, ignore_collections, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[39mfor\u001b[39;00m i, dict_tuple \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(value\u001b[39m.\u001b[39mneural_type\u001b[39m.\u001b[39melements_type\u001b[39m.\u001b[39mtype_parameters\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m    218\u001b[0m         error_msg\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m  input param_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m{\u001b[39;00mdict_tuple[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdict_tuple[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msg))\n\u001b[1;32m    221\u001b[0m \u001b[39m# Perform input ndim check\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: NeuralTypeComparisonResult.INCOMPATIBLE :\nInput type expected : axes: (batch, time); elements_type: Index\nInput type found : axes: (batch, time); elements_type: LabelsType\nArgument: x"
     ]
    }
   ],
   "source": [
    "# Attach wrong neural type [ERROR CELL]\n",
    "x1.neural_type = NeuralType(('B','T'), LabelsType())\n",
    "print(\"embedding(x): \", embedding_module(x=x1).shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the minGPT components\n",
    "\n",
    "Now that we have somewhat firm grasp of neural type checking, let's begin porting the minGPT example code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Element types\n",
    "\n",
    "Till now , we have used the Neural Types provided by the NeMo core. But we need not restricted to the pre-defined element types !\n",
    "Users have total flexibility in defining hierarchy of element types as they please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionType(EncodedRepresentation):\n",
    "    \"\"\"Basic Attention Element Type\"\"\"\n",
    "\n",
    "class SelfAttentionType(AttentionType):\n",
    "    \"\"\"Self attention element type\"\"\"\n",
    "\n",
    "class CausalSelfAttentionType(SelfAttentionType):\n",
    "    \"\"\"Causal Self Attention Element Type\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the modules\n",
    "\n",
    "Neural Module are generally top-level modules but can be used at any level of the module hierarchy\n",
    "\n",
    "For demonstration, we will treat an encoder comprising a block of Causal Self Attention modules as a typed Neural Module. Of course, we can also treat each causal self attention layer itself as a neural module if we require it, but top-level modules are generally prefered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use a torch.nn.MultiHeadAttention here but I am including an \n",
    "    explicit implementation here to show that there is nothing to scary here\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, block_size, n_head, attn_pdrop, resid_pdrop) -> None:\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        # Regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                        .view(1, 1, block_size, block_size))\n",
    "        \n",
    "    def forward(self, x, layer_mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T]) == 0, float('-inf')\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    '''An unassuming Transformer block'''\n",
    "\n",
    "    def __init__(self, n_embd, block_size, n_head, attn_pdrop, resid_pdrop) -> None:\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, block_size, n_head, attn_pdrop, resid_pdrop)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU,\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NeMo Model\n",
    "\n",
    "Let's start by inheriting `ModelPT` - the core class of a PyTorch NeMo model, which inherits the PyTorch Lightning Module.\n",
    "\n",
    "- The NeMo equivalent of `torch.nn.Module` is the NeuralModule\n",
    "- The NeMo equivalent of the `LightningModule` is `ModelPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-23 10:57:28 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as ptl\n",
    "from nemo.core import ModelPT\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the bare minimum implementation of the NeMo model - just the constructor, the initializer of weights, and the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTLGPT(ptl.LightningModule):\n",
    "    def __init__(self,\n",
    "                #  Model definition args\n",
    "                vocab_size: int, # size of the vocabulary (number of possible toker)\n",
    "                block_size: int, # length of the model's context window in time\n",
    "                n_layer: int, # depth of the model; number of transformer blocks in sequence\n",
    "                n_embd: int, # the \"width\" of the model, number of each channels in each transformer\n",
    "                n_head: int, # number of heads in each multi-head attention inside \n",
    "                # model optimization args\n",
    "                learning_rate: float = 3e-4, # the base learning rate of the model\n",
    "                weight_decay: float = 0.1, # amount of regularizing L2 weight decay on MatMul ops\n",
    "                betas: Tuple[float, float] = (0.9, 0.95), # momentum terms (betas) for the Adam optimizer\n",
    "                embd_prop: float = 0.1, # \\in [0,1]: amount of dropout on input embedding\n",
    "                resid_pdrop: float = 0.1, # \\in [0,1]:amount of dropout in each residual connection\n",
    "                attn_pdrop: float = 0.1, # \\in [0,1]:amount of dropout on the attention matrix\n",
    "                ):\n",
    "        super.__init__()\n",
    "\n",
    "        # save these for optimizer init later\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.betas = betas\n",
    "\n",
    "        # input embedding stem: drop(content+position)\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.drop = nn.Dropout(embd_prop)\n",
    "        # deep transformer: just a sequence of trandformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, block_size, n_head, attn_pdrop, resid_pdrop) for _ in range(n_layer)])\n",
    "        # decoder: at the end one more layernorm and decode the answers\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False) # no need for extra bias due to one in ln_f\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(\"number of parameters: %e\" % sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embedding = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embedding = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embedding + position_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Vanilla model initialization :\n",
    "        - all matmul weights \\in N(0, 0.02) and biases to zero \n",
    "        - all LayerNorm post-normalization scaling set to identit, so weight=1 bias=0\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor '__init__' of 'super' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m \u001b[39m=\u001b[39m PTLGPT(vocab_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, block_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, n_layer\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, n_embd\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, n_head\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mPTLGPT.__init__\u001b[0;34m(self, vocab_size, block_size, n_layer, n_embd, n_head, learning_rate, weight_decay, betas, embd_prop, resid_pdrop, attn_pdrop)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m      3\u001b[0m             \u001b[39m#  Model definition args\u001b[39;00m\n\u001b[1;32m      4\u001b[0m             vocab_size: \u001b[39mint\u001b[39m, \u001b[39m# size of the vocabulary (number of possible toker)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m             attn_pdrop: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m, \u001b[39m# \\in [0,1]:amount of dropout on the attention matrix\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             ):\n\u001b[0;32m---> 17\u001b[0m     \u001b[39msuper\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n\u001b[1;32m     19\u001b[0m     \u001b[39m# save these for optimizer init later\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m learning_rate\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor '__init__' of 'super' object needs an argument"
     ]
    }
   ],
   "source": [
    "m = PTLGPT(vocab_size=100, block_size=32, n_layer=1, n_embd=32, n_head=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
